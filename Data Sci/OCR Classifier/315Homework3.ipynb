{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smart_dict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLines(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            lines.append(line.replace('\\n', ''))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFeatures(lines,data_tokens,data_dict):\n",
    "    features = np.zeros(shape=(len(lines),len(data_tokens)))\n",
    "    i = 0\n",
    "    for sentence in lines:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word in data_dict:\n",
    "                features[i,data_dict[word]] = 1\n",
    "        i = i + 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLabels(path):\n",
    "    labels = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            labels.append(line.replace('\\n', '').replace('0',\"-1\") )\n",
    "        \n",
    "    labels = np.asarray(labels, dtype=int)  \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Perceptron\n",
    "class Perceptron(object):\n",
    "    def __init__(self, no_of_inputs, training_inputs, training_labels,testing_inputs, testing_labels,threshold=20, learning_rate=1):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterTrainAccuracy = []\n",
    "        self.iterTestAccuracy = []\n",
    "        self.iterTrainMistakes = []\n",
    "        self.iterTestMistakes = []\n",
    "        self.training_data = training_inputs\n",
    "        self.training_labels = training_labels\n",
    "        self.testing_data = testing_inputs\n",
    "        self.testing_labels = testing_labels\n",
    "        self.weights = np.zeros(no_of_inputs)\n",
    "    \n",
    "    def predict(self, inputs):    \n",
    "        return np.sign(np.dot(inputs, self.weights)) \n",
    "    \n",
    "    def finalTrain(self):\n",
    "        return self.iterTrainAccuracy[-1]\n",
    "        \n",
    "    def finalTest(self):\n",
    "        return self.iterTestAccuracy[-1]\n",
    "    \n",
    "    def test(self):\n",
    "        mistakes = 0\n",
    "        for inputs, label in zip(self.testing_data, self.testing_labels):\n",
    "            prediction = self.predict(inputs)\n",
    "            if(prediction != label):\n",
    "                mistakes += 1\n",
    "        self.iterTestMistakes.append(mistakes)\n",
    "        self.iterTestAccuracy.append((len(self.testing_data)-mistakes)/len(self.testing_data))\n",
    "        \n",
    "    def train(self):\n",
    "        for _ in range(self.threshold):\n",
    "            mistakes = 0\n",
    "            for inputs, label in zip(self.training_data, self.training_labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                if(prediction != label):\n",
    "                    mistakes+=1\n",
    "                    self.weights += self.learning_rate * (label) * inputs\n",
    "                \n",
    "            self.test()      \n",
    "            self.iterTrainMistakes.append(mistakes)\n",
    "            self.iterTrainAccuracy.append((len(self.training_data)-mistakes)/len(self.training_data))\n",
    "        return self.weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Averaged Perceptron\n",
    "class aperceptron(object): \n",
    "    def __init__(self,no_of_inputs, training_inputs, training_labels,threshold=20, learning_rate=1):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs)\n",
    "        self.u = np.zeros(no_of_inputs)\n",
    "        self.bias = 0\n",
    "        self.beta = 0\n",
    "        self.training_data = training_inputs\n",
    "        self.training_labels = training_labels\n",
    "        self.count = 1\n",
    "        self.trainingAccuracy = 0\n",
    "        self.testingAccuracy = 0\n",
    "        \n",
    "    def predict(self,inputs,label):\n",
    "        return np.sign(np.dot(inputs, self.weights) + self.bias)\n",
    "        \n",
    "    def train(self):\n",
    "    #Average perceptron algorithm\n",
    "        for _ in range(self.threshold):\n",
    "            mistakes = 0\n",
    "            for  inputs, label in zip(self.training_data, self.training_labels):\n",
    "                prediction = self.predict(inputs,label)\n",
    "                if (prediction != label):\n",
    "                    mistakes+=1\n",
    "                    self.weights += label * inputs * self.learning_rate\n",
    "                    self.bias += label * self.learning_rate\n",
    "                    self.u += label * self.count * inputs * self.learning_rate\n",
    "                    self.beta +=  label * self.count * self.learning_rate\n",
    "            self.count = self.count + 1\n",
    "            self.trainingAccuracy = ((len(self.training_data)-mistakes)/len(self.training_data))\n",
    "        self.weights = self.weights - (1/self.count)*self.u\n",
    "        self.bias = np.array([self.bias- (1/self.count)*self.beta]) \n",
    "        return self.weights\n",
    "    \n",
    "    def testAccuracy(self,testing_inputs,testing_labels):\n",
    "        mistakes = 0\n",
    "        for inputs, label in zip(testing_inputs, testing_labels):\n",
    "            prediction = self.predict(inputs,label)\n",
    "            if prediction <= 0:\n",
    "                mistakes += 1\n",
    "        self.testingAccuracy = ((len(testing_inputs)-mistakes)/len(testing_inputs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1---------------------------------------------------------------------\n",
    "\n",
    "#READ AND PARSE STOPLIST--------------------------\n",
    "with open(\"stoplist.txt\") as file:\n",
    "    stop_words = file.read().replace('\\n', ' ') \n",
    "\n",
    "stopwords = stop_words.split()\n",
    "stop_words_dict = Counter(stop_words.split())\n",
    "#-------------------------------------------------\n",
    "\n",
    "#READ AND ESTABLISH VOLCABULARY-------------------\n",
    "with open('traindata.txt', 'r') as file:\n",
    "    text = file.read().replace('\\n', ' ')  \n",
    "\n",
    "token = set(' '.join([word for word in text.split() if word not in stop_words_dict]).split())\n",
    "training_data_tokens = sorted(token)\n",
    "\n",
    "training_data_dict = {}\n",
    "i = 0\n",
    "for entry in training_data_tokens:\n",
    "    training_data_dict[entry] = i \n",
    "    i = i + 1\n",
    "#-------------------------------------------------\n",
    "\n",
    "#CONSTRUCT TRAINING DATA AND LABELS---------------\n",
    "lines = parseLines('traindata.txt')\n",
    "training_labels = parseLabels('trainlabels.txt') \n",
    "features = parseFeatures(lines,training_data_tokens,training_data_dict)\n",
    "#-------------------------------------------------\n",
    "\n",
    "#CONSTRUCT TESTING DATA AND LABELS----------------\n",
    "lines = parseLines('testdata.txt')\n",
    "testing_labels = parseLabels('testlabels.txt')\n",
    "testfeatures = parseFeatures(lines,training_data_tokens,training_data_dict)\n",
    "#-------------------------------------------------\n",
    "\n",
    "perceptron = Perceptron(features.shape[1],features, training_labels,testfeatures,testing_labels)\n",
    "perceptron.train()\n",
    "\n",
    "Aperceptron = aperceptron(features.shape[1],features, training_labels)\n",
    "Aperceptron.train()\n",
    "Aperceptron.testAccuracy(testfeatures,testing_labels)\n",
    "\n",
    "#PART 2---------------------------------------------------------------------\n",
    "\n",
    "vowel_dict = smart_dict({'a':1,'e':1,'i':1,'o':1,'u':1})\n",
    "\n",
    "def readIn(path):\n",
    "    data = pd.read_table(path, delim_whitespace=True,names = ('Num','im','label','del'),dtype={'Num': np.int64, 'im': str, 'label': type('')})\n",
    "    del data['Num']\n",
    "    del data['del']\n",
    "    data['im'] = data['im'].map(lambda x: x.lstrip('im'))\n",
    "    return data\n",
    "\n",
    "d = readIn('ocr_train.txt')\n",
    "ocr_training_labels = np.array([vowel_dict[x] for x in d['label']], dtype=int)\n",
    "ocr_training_features = np.array([list(x) for x in d['im']], dtype=int)\n",
    "\n",
    "d = readIn('ocr_test.txt')\n",
    "ocr_testing_labels = np.array([vowel_dict[x] for x in d['label']], dtype=int)\n",
    "ocr_testing_features = np.array([list(x) for x in d['im']], dtype=int)\n",
    "\n",
    "perceptronOCR = Perceptron(ocr_training_features.shape[1],ocr_training_features, ocr_training_labels,ocr_testing_features,ocr_testing_labels)\n",
    "perceptronOCR.train()\n",
    "\n",
    "AperceptronOCR = aperceptron(ocr_training_features.shape[1],ocr_training_features, ocr_training_labels)\n",
    "AperceptronOCR.train()\n",
    "AperceptronOCR.testAccuracy(ocr_testing_features,ocr_testing_labels)\n",
    "\n",
    "with open(\"output.txt\", \"w\") as text_file:\n",
    "    print(\"FORTUNE COOKIE DATA\\n\",file = text_file)\n",
    "    for i in range(0,len(perceptron.iterTrainMistakes)):\n",
    "        print(\"iteration-\" , (i+1) , \" train-mistakes: \" , perceptron.iterTrainMistakes[i] , \" test-mistakes: \" , perceptron.iterTestMistakes[i]  ,file = text_file)\n",
    "    print(\"\\n\",file = text_file)\n",
    "    for i in range(0,len(perceptron.iterTrainAccuracy)):\n",
    "        print(\"iteration-\" , (i+1) , \" training-Accuracy :\" , perceptron.iterTrainAccuracy[i] , \" testing-Accuracy: \" , perceptron.iterTestAccuracy[i]  ,file = text_file)\n",
    "        \n",
    "    print(\"\\ntraining-accuracy-standard-perceptron: \", perceptron.finalTrain(), \" training-accuracy-averaged-perceptron: \" , Aperceptron.trainingAccuracy,file = text_file)\n",
    "    print(\"testing-accuracy-standard-perceptron: \",perceptron.finalTest(), \" testing-accuracy-averaged-perceptron: \" , Aperceptron.testingAccuracy,file = text_file)\n",
    "    print(\"\\n\",file = text_file)\n",
    "    print(\"\\nOCR DATA\\n\",file = text_file)\n",
    "    for i in range(0,len(perceptronOCR.iterTrainMistakes)):\n",
    "        print(\"iteration-\" , (i+1) , \" train-mistakes: \" , perceptronOCR.iterTrainMistakes[i] , \" test-mistakes: \" , perceptronOCR.iterTestMistakes[i]  ,file = text_file)\n",
    "    print(\"\\n\",file = text_file)\n",
    "    for i in range(0,len(perceptronOCR.iterTrainAccuracy)):\n",
    "        print(\"iteration-\" , (i+1) , \" training-Accuracy :\" , perceptronOCR.iterTrainAccuracy[i] , \" testing-Accuracy: \" , perceptronOCR.iterTestAccuracy[i]  ,file = text_file)\n",
    "        \n",
    "    print(\"\\ntraining-accuracy-standard-perceptron: \", perceptronOCR.finalTrain(), \" training-accuracy-averaged-perceptron: \" , AperceptronOCR.trainingAccuracy,file = text_file)\n",
    "    print(\"testing-accuracy-standard-perceptron: \",perceptronOCR.finalTest(), \" testing-accuracy-averaged-perceptron: \" , AperceptronOCR.testingAccuracy,file = text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-e98bb0546419>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-e98bb0546419>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Inputs = the learning rate N,  the training threshold T, the input feature vector candidate pairs P and the sign of the corresponding candidate scores L (in the same order as P)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Inputs = the learning rate N,  the training threshold T, the input feature vector candidate pairs P and the sign of the corresponding candidate scores L (in the same order as P)\n",
    "Output: w, the final weight vector.\n",
    "1. init the weights w = 0\n",
    "2. init the bias = 0\n",
    "3. for each training iteration _ in T do\n",
    "4.    for each pair (kt,lt) , yt in zip(P,L)\n",
    "5.        prediction = sign((k dot w + bias) - (l dot w + bias))\n",
    "6.        if prediction != yt then \n",
    "7.        w = w + N * yt  * P\n",
    "8.        bias = bias + N * yt\n",
    "9.        end if\n",
    "10.    end for\n",
    "11. end for\n",
    "12.return the weight vector w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
